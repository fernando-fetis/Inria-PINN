{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbatched input:\n",
      "Output shape: torch.Size([25, 2])\n",
      "Hidden shape: torch.Size([5, 2])\n",
      "\n",
      "Batched input:\n",
      "Output shape: torch.Size([25, 100, 2])\n",
      "Hidden shape: torch.Size([5, 100, 2])\n"
     ]
    }
   ],
   "source": [
    "# Architecture parameters and instantiation:\n",
    "input_size = 10\n",
    "hidden_size = 2\n",
    "num_layers = 5\n",
    "\n",
    "rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Data parameters:\n",
    "sequence_length = 25\n",
    "batch_size = 100\n",
    "\n",
    "# Unbatached forward:\n",
    "sequence = torch.randn(sequence_length, input_size)\n",
    "initial_hidden = torch.randn(num_layers, hidden_size)\n",
    "output, hidden = rnn(sequence, initial_hidden)\n",
    "\n",
    "print('Unbatched input:')\n",
    "print('Output shape:', output.shape)\n",
    "print('Hidden shape:', hidden.shape)\n",
    "\n",
    "# Batched forward:\n",
    "batch_seq = torch.randn(sequence_length, batch_size, input_size)\n",
    "initial_hidden = torch.randn(num_layers, batch_size, hidden_size)\n",
    "output, hidden = rnn(batch_seq, initial_hidden)\n",
    "\n",
    "print('\\nBatched input:')\n",
    "print('Output shape:', output.shape)\n",
    "print('Hidden shape:', hidden.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.shape = [seq_length, batch_size, input_size]\n",
    "        # By default, initial hidden state of a RNN forward will be zero if not provided.\n",
    "        output, hidden = self.rnn(input)  # output.shape = [seq_length, batch_size, hidden_size].\n",
    "        output = self.classifier(output[-1])  # output.shape = [batch_size, num_classes].\n",
    "        return nn.Softmax(dim=1)(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape: torch.Size([100, 6])\n"
     ]
    }
   ],
   "source": [
    "input_size = 10\n",
    "hidden_size = 2\n",
    "num_layers = 5\n",
    "num_classes = 6\n",
    "\n",
    "classifier = RNNClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "sequence_length = 25\n",
    "batch_size = 100\n",
    "batch_seq = torch.randn(sequence_length, batch_size, input_size)\n",
    "\n",
    "classes_prediction = classifier(batch_seq)\n",
    "\n",
    "print('Prediction shape:', classes_prediction.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([35, 100, 20])\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, hidden = self.rnn(input)\n",
    "        return output, hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(output_size, hidden_size, num_layers)\n",
    "        self.output_fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, h0):\n",
    "        # In autoregressive models (like this one), the length of the input sequence must be 1.\n",
    "        output, hidden = self.rnn(input, h0)\n",
    "        # Thus, output.shape = [1, batch_size, hidden_size] and it can be squeezed before the linear layer forward.\n",
    "        output = self.output_fc(output.squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder and decoder hidden sizes must be equal. Same for the number of layers.\n",
    "        self.encoder = Encoder(input_size, hidden_size, num_layers)\n",
    "        self.decoder = Decoder(output_size, hidden_size, num_layers)\n",
    "\n",
    "\n",
    "    def forward(self, input, target=None, teaching_forcing=0, max_length=30):\n",
    "        \n",
    "        # input.shape = [input_seq_length, batch_size, input_size]\n",
    "        # target.shape = [output_seq_length, batch_size, output_size] or None in testing.\n",
    "        \n",
    "        # teaching_forcing indicates the probability to use real target as input in the decoder instead of\n",
    "        # the previous output (non-autoregressive mode). Must be zero when testing.\n",
    "        \n",
    "        if teaching_forcing > 0:\n",
    "            assert target is not None, 'There is no target for using teaching forcing.'\n",
    "        \n",
    "        outputs = []  # will save the output vector at each time (prediction at time t, for each t).\n",
    "        _, encoder_hidden = self.encoder(input)  # encoder_hidden.shape = [num_layers, batch_size, hidden_size].\n",
    "        \n",
    "        \n",
    "        # The input of the decoder at t=0 will be a null tensor by default (<SOS> in the literature).\n",
    "        # input_decoder.shape = [1, batch_size, dec_input_size=output_size] (simulation a seq. of length 1):\n",
    "        # The last hidden states of the encoder are the initial hidden states in the decoder.\n",
    "        \n",
    "        output_decoder, hidden_decoder = torch.zeros(input.shape[1], self.decoder.rnn.input_size), encoder_hidden\n",
    "        \n",
    "        \n",
    "        output_length = len(target) if target is not None else max_length\n",
    "        for t in range(output_length):\n",
    "            # output_decoder.shape = [batch_size, output_size].\n",
    "            # hidden_decoder.shape = [num_layers, batch_size, hidden_size].\n",
    "            \n",
    "            teacher_forcing = random.random() < teaching_forcing  # to use or not teaching forcing.\n",
    "            input_decoder = target[t] if teacher_forcing else output_decoder  # teacher forcing uses the real (previous) target as input in the decoder.\n",
    "            input_decoder = input_decoder.unsqueeze(0)\n",
    "            \n",
    "            output_decoder, hidden_decoder = self.decoder(input_decoder, hidden_decoder)\n",
    "            outputs.append(output_decoder)\n",
    "            \n",
    "        return torch.stack(outputs, dim=0)\n",
    "\n",
    "input_size = 10\n",
    "output_size = 20\n",
    "hidden_size = 2\n",
    "num_layers = 5\n",
    "\n",
    "model = Seq2Seq(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "input_seq_length = 25\n",
    "target_seq_length = 35\n",
    "batch_size = 100\n",
    "\n",
    "# Evaluation mode:\n",
    "input = torch.randn(input_seq_length, batch_size, input_size)\n",
    "target = torch.randn(target_seq_length, batch_size, output_size)\n",
    "output = model(input, target=target, teaching_forcing=0.5)\n",
    "print('Output shape:', output.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder model for PINNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): # ! probar que el número de capas sea el número de variables hidden.\n",
    "    \n",
    "    def __init__(self, n_input_vars, n_hidden_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        \"\"\"Encodes a group of points. Points will have static variables (representing the initial hidden\n",
    "        state of the RNN) and sequential variables (passing through the RNN sequentially).\n",
    "\n",
    "        Args:\n",
    "            n_input_vars (int): number of variables to input rnn (for each time).\n",
    "            n_hidden_vars (int): number of variables to initial hidden state.\n",
    "            rnn_input_size (int): input rnn dimension (for each tame).\n",
    "            rnn_hidden_size (int): rnn hidden state dimension.\n",
    "            num_layers (int): number of layers for the RNN.\n",
    "        \"\"\"        \n",
    "        super().__init__()\n",
    "        self.hidden_fc = nn.Linear(n_hidden_vars, rnn_hidden_size)\n",
    "        self.input_fc = nn.Linear(n_input_vars, rnn_input_size)\n",
    "        self.rnn = nn.RNN(rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self, input, h0):\n",
    "        \"\"\"Performs an affine transformation of both set of points (in order to allow a larger RNN cell).\n",
    "        They are then passed through an RNN and the outputs are returned.\n",
    "\n",
    "        Args:\n",
    "            input (tensor[seq_length, batch_size, n_input_vars]): batch of sequences of points sampled from input variables.\n",
    "            h0 (tensor[batch_size, n_hidden_vars]): batch of fixed points samples from hidden variables (one tuple per batch sequence).\n",
    "\n",
    "        Returns:\n",
    "            output_rnn (tensor[[seq_length, batch_size, rnn_hidden_size]): last layer outputs (for each time).\n",
    "            hidden_rnn (tensor[num_layers, batch_size, rnn_hidden_size]): last hidden states (for each layer).\n",
    "        \"\"\"        \n",
    "        hidden_rnn = self.hidden_fc(h0.expand)  # shape: [batch_size, rnn_hidden_size].\n",
    "        input_rnn = self.input_fc(input.view(batch_size, -1))  # shape: [, rnn_input_size]\n",
    "        output_rnn, hidden_rnn = self.rnn(input_rnn, hidden_rnn)\n",
    "        return output_rnn, hidden_rnn\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_input_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(n_input_vars, rnn_input_size)\n",
    "        self.rnn = nn.RNN(rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        self.output_fc = nn.Linear(rnn_hidden_size, n_input_vars)\n",
    "        \n",
    "    def forward(self, input, h0):     \n",
    "        input = self.input_fc(input)\n",
    "        output, hidden = self.rnn(input, h0)\n",
    "        output = self.output_fc(output.squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class EDPINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vars, hidden_vars, target_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_vars = input_vars\n",
    "        self.hidden_vars = hidden_vars\n",
    "        self.target_vars = target_vars\n",
    "        \n",
    "        self.encoder = Encoder(input_vars, hidden_vars, rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        self.decoder = Decoder(target_vars, rnn_input_size, rnn_hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input_points, hidden_points):\n",
    "        seq_length, n_samples, input_vars = input_points.shape        \n",
    "        outputs = torch.empty(seq_length, n_samples, self.target_vars)\n",
    "        \n",
    "        # Encoder:\n",
    "        _, encoder_hidden = self.encoder(input_points, hidden_points)\n",
    "        \n",
    "        # Decoder:\n",
    "        input_decoder, hidden_decoder = torch.zeros(self., n_hidden_vars), encoder_hidden\n",
    "        for t in range(seq_length):\n",
    "            input_decoder = input_decoder.unsqueeze(0)\n",
    "            input_decoder, hidden_decoder = self.decoder(input_decoder, hidden_decoder)\n",
    "            outputs[t] = input_decoder\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 100, 1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data generation:\n",
    "\n",
    "# ! dejar al tiempo en la variable secuencial puede permitir estudiar la pinn hasta tiempos más largos.\n",
    "\n",
    "input_vars = 1\n",
    "hidden_vars = 2\n",
    "target_vars = 1\n",
    "rnn_input_size = 5\n",
    "rnn_hidden_size = 10\n",
    "num_layers = 1 # arreglar.\n",
    "\n",
    "net = AttentionPINN(input_vars, hidden_vars, target_vars, rnn_input_size, rnn_hidden_size, num_layers)\n",
    "\n",
    "n_samples = 100\n",
    "seq_length = 20\n",
    "\n",
    "# num_layers, batch_size, hidden_size\n",
    "input_points = torch.randn(seq_length, n_samples, input_vars)\n",
    "hidden_points = torch.randn(n_samples, hidden_vars)\n",
    "\n",
    "net(input_points, hidden_points).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 10\n",
    "# output_size = 20\n",
    "# hidden_size = 1\n",
    "# num_layers = 5\n",
    "\n",
    "# model = Seq2Seq(input_size, output_size, hidden_size, num_layers)\n",
    "\n",
    "# input_seq_length = 25\n",
    "# target_seq_length = 35\n",
    "# batch_size = 100\n",
    "\n",
    "# # Evaluation mode:\n",
    "# input = torch.randn(input_seq_length, batch_size, input_size)\n",
    "# target = torch.randn(target_seq_length, batch_size, output_size)\n",
    "# output = model(input, target=target, teaching_forcing=0.5)\n",
    "# print('Output shape:', output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vars, hidden_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_fc = nn.Linear(hidden_vars, rnn_hidden_size)\n",
    "        self.input_fc = nn.Linear(input_vars, rnn_input_size)\n",
    "        self.rnn = nn.RNN(rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        \n",
    "    def forward(self, input_points, hidden_points):\n",
    "        hidden_points = self.hidden_fc(hidden_points).unsqueeze(0)\n",
    "        input_points = self.input_fc(input_points)\n",
    "        output, hidden = self.rnn(input_points, hidden_points)\n",
    "        return output, hidden\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_fc = nn.Linear(target_vars, rnn_input_size)\n",
    "        self.rnn = nn.RNN(rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        self.output_fc = nn.Linear(rnn_hidden_size, target_vars)\n",
    "        \n",
    "    def forward(self, input, h0):\n",
    "        input = self.input_fc(input)\n",
    "        output, hidden = self.rnn(input, h0)\n",
    "        output = self.output_fc(output.squeeze(0))\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class EDPINN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_vars, hidden_vars, target_vars, rnn_input_size, rnn_hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.input_vars = input_vars\n",
    "        self.hidden_vars = hidden_vars\n",
    "        self.target_vars = target_vars\n",
    "        \n",
    "        self.encoder = Encoder(input_vars, hidden_vars, rnn_input_size, rnn_hidden_size, num_layers)\n",
    "        self.decoder = Decoder(target_vars, rnn_input_size, rnn_hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input_points, hidden_points):\n",
    "        seq_length, n_samples, input_vars = input_points.shape        \n",
    "        outputs = torch.empty(seq_length, n_samples, self.target_vars)\n",
    "        \n",
    "        # Encoder:\n",
    "        _, encoder_hidden = self.encoder(input_points, hidden_points)\n",
    "        \n",
    "        # Decoder:\n",
    "        input_decoder, hidden_decoder = torch.zeros(self., n_hidden_vars), encoder_hidden\n",
    "        for t in range(seq_length):\n",
    "            input_decoder = input_decoder.unsqueeze(0)\n",
    "            input_decoder, hidden_decoder = self.decoder(input_decoder, hidden_decoder)\n",
    "            outputs[t] = input_decoder\n",
    "            \n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
