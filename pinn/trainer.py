import torch
import torch.nn as nn
import torch.optim as optim
from pinn import pde, neuralnetworks
import matplotlib.pyplot as plt
import numpy as np
import tqdm
import scipy.interpolate as interpolate
import scipy.stats as stats
from pinn.pcgrad import PCGrad


class Trainer:
    
    
    def __init__(self,
                 pde: pde,
                 net: neuralnetworks,
                 optimizer: optim.Optimizer,
                 scheduler: optim.lr_scheduler = None,
                 pcgrad: bool = False):
        
        '''
        Initialize the Trainer class. It needs the following parameters:
        
        - pde [pde]: object containing all information about the pde.
        - net [neuralnetworks]: nn.Module modified object to be used as a PINN.
        - optimizer [Optimizer]: PyTorch optimizer associated to net.
        - scheduler [lr_scheduler]: PyTorch scheduler used to control the learning rate.
        - pcgrad [bool]: indicates if PCGrad will be used or not.
        '''     
        
        self.pde = pde
        self.net = net
        self.pcgrad = pcgrad
        self.optimizer = PCGrad(optimizer) if pcgrad else optimizer
        self.scheduler = scheduler        
        
        # Default:
        self.loss_fn = nn.MSELoss()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # Training statistics:
        self.losses = {'physics': [], 'boundary': [], 'initial': [], 'known': [], 'total': []}
        self.best_state = {'state': None, 'epoch': 0, 'loss': float('inf')}
    
    
    def _generate_data(self, samples: dict):
        '''
        Protected method used to sampling data points (and their respective targets). Requires:
        
        - samples [dict]: in the form {'physics': int, 'boundary': int, 'initial': int, 'known': int}.
            each key will indicates the number of points sampled for each type of loss.
            
        returns:
        - data [dict]: contains the input of each sample generated by the method.
        - normal [dict]: contains the normal direction of each point in the boundary. Used to calculate
            normal derivatives in boundary conditions.
        - target [dict]: containts the target used to compare the final output of each data type.
        '''
        
        data, normal, target = {}, {}, {}
        
        # Each data type needs a callable to be used after the PINN forward and before the loss calculation:
        needed_callables = {'physics': 'equation',
                            'boundary': 'boundary_condition',
                            'initial': 'initial_condition',
                            'known': 'known_solution'}
        
        for data_type in ('physics', 'boundary', 'initial', 'known'):
            
            # Data will be generated according to samples[data_type] as long as the needed callable is defined:
            if samples.get(data_type, 0) > 0  and getattr(self.pde, needed_callables[data_type]) is not None:
                data[data_type], normal[data_type] = self.pde.domain.sample(
                     k_samples=samples[data_type],
                     at_initial = (data_type == 'initial'),
                     on_boundary = (data_type == 'boundary'),
                     device=self.device)
                
                if data_type == 'known':
                    # known_solution returns a tuple with solutions (one element for each output) to compare
                    # each output of the network in each subdomain.
                    target[data_type] = {subdomain: self.pde.known_solution(data_subdomain)
                                        for subdomain, data_subdomain in data[data_type].items()}
                else:
                    # All the others constraints will be compared with zero (it is assumed that the constraints
                    # are in the form f(u, data) = 0) therefore, only one target is needed for each subdomain.
                    # Because of this, self._calc_loss must to calculate the loss associated to known data in a
                    # different way.
                    first_var_name = self.pde.domain.variable_names[0]
                    target[data_type] = {subdomain: torch.zeros(len(data_subdomain[first_var_name]), device=self.device)
                                        for subdomain, data_subdomain in data[data_type].items()}
            else:
                data[data_type], target[data_type] = None, None
        return data, normal, target
            

    def _calc_loss(self, full_data: tuple, weights: dict, save_loss: bool = False):
        '''
        Protected method used to calculate the loss in the optimization. It requires the following parameters:
        
        - full_data [tuple]: tuple containing the data (input), normals of each point in data and the targets of each point in data.
            This tuple is generated by self._generate_data.
        - weights [dict]: indicates the ponderation used to get a unique loss when PCGrad is disabled. It has the
            form {'physics': float, 'boundary': float, 'initial': float, 'known': float}.
        - save_loss [bool]: if True, the loss calculated will be saved in self.losses. It is not always necessary to do
            because this method is called several times by second-order optimizers in a iteration.
            
        returns:
        - list with each type of loss if PCGrad is enabled. Else, returns the total loss generated by the ponderated sum
            of each loss using weights.
        '''
        
        data, normal, target = full_data
        u, outputs, losses = {}, {}, {}
        
        for data_type in ('physics', 'boundary', 'initial', 'known'):
            
            if data[data_type] is not None:
            
                # Net forward (solutions prediction):
                u[data_type] = {subdomain: self.net(data_subdomain) for subdomain, data_subdomain in data[data_type].items()}
            
                # Outputs of the model (to compare with the targets). Each data type has a different callable. Each
                # output will have the form {subdomain: (output_1, ..., output_n)} with n the number of constraints
                # associated to the loss type (number of equations, boundary conditions, etc.).
                match data_type:
                    case 'physics':
                        outputs[data_type] = {subdomain: self.pde.equation(data_subdomain, u[data_type][subdomain])
                                            for subdomain, data_subdomain in data[data_type].items()}
                    case 'boundary':
                        outputs[data_type] = {subdomain: self.pde.boundary_condition[subdomain](data_subdomain, normal[data_type][subdomain], u[data_type][subdomain])
                                            for subdomain, data_subdomain in data[data_type].items()}
                    case 'initial':
                        outputs[data_type] = {subdomain: self.pde.initial_condition(data_subdomain, u[data_type][subdomain])
                                            for subdomain, data_subdomain in data[data_type].items()}
                    case 'known':
                        outputs[data_type] = {subdomain: u[data_type][subdomain]
                                              for subdomain, data_subdomain in data[data_type].items()}
                        #! outputs['known'] == u['known']?
                
                # Losses calculation:
                if data_type != 'known':
                    # For each subdomain, a unique target (zero tensor) will be used to compare each element in the
                    # tuple output[data_type][subdomain].
                    losses[data_type] = sum([self.loss_fn(eq_output, subdomain_target)
                                            for subdomain, subdomain_target in target[data_type].items()
                                            for eq_output in outputs[data_type][subdomain]])
                else:
                    # For each element in the tuple output[data_type][subdomain] there is a specific target to compare.
                    # Targets are defined in the tuple target[data_type][subdomain] according to self._generate_data.
                    losses[data_type] = sum([self.loss_fn(eq_output, subdomain_target[n_eq])
                                            for subdomain, subdomain_target in target[data_type].items()
                                            for n_eq, eq_output in enumerate(outputs[data_type][subdomain])
                                            if subdomain_target[n_eq] is not None])
            else:
                losses[data_type] = torch.zeros(1)
            
        total_loss = sum([weights[data_type] * losses[data_type] for data_type in ('physics', 'boundary', 'initial', 'known')])
        
        if save_loss:
            for data_type in ('physics', 'boundary', 'initial', 'known'):
                self.losses[data_type].append(weights[data_type] * losses.get(data_type, 0).item())
            self.losses['total'].append(total_loss.item())
            
        return list(losses.values()) if self.pcgrad else total_loss
    
    
    def train(self,
              epochs: int,
              samples: dict = {'physics': 500, 'boundary': 500, 'initial': 500, 'known': 1000},
              weights: dict = {'physics': 1, 'boundary': 1, 'initial': 1, 'known': 9},
              ci_config: dict = {'confidence': 0.95, 'epochs': 30, 'step': 0.1, 'dropout': 0.01},
              calculate_confidence: bool = False,
              early_stopping: int = False):
        
        '''
        Training method. It requires the following args:
        
        - epochs [int]: number of epochs of training.
        - samples [dict]: dictionary used for sampling data with self._generate_data.
        - weights [dict]: dictionary used in self._calc_loss.
        - ci_config [dict]: configuration needed for calculate confidence intervals using dropout (see references in the report).
        - calculate_confidence [bool]: flag indicating if the confidence will be calculated or not.
        - early_stopping [int]: patience of the trainer until stopping because the model does not improve.
        '''
        
        def closure():
            '''
            Inner method used to clean gradients and make the backward step. It is used by the optimizers.
            '''
            self.optimizer.zero_grad()
            loss = self._calc_loss(train_data, weights)
            
            if self.pcgrad:
                self.optimizer.pc_backward(loss)
                return sum(loss) # ! est√° bien esto?
            else:
                loss.backward(retain_graph=True)
                return loss
            
        # Training:
        self.net.train()
        try:
            train_data = self._generate_data(samples)
            
            # Progress bar:
            bar_format = '{l_bar}{bar}| epoch {n_fmt} of {total_fmt} ({rate_fmt}) | loss: {postfix} | training time: {elapsed}'
            epoch_counter = tqdm.trange(1, epochs + 1, unit='epochs', bar_format=bar_format, miniters=1)
            
            patience, loss = 0, 0
            
            for epoch in epoch_counter:
                
                # Optimization:
                self.optimizer.step(closure)
                if self.scheduler is not None:
                    self.scheduler.step()
                
                loss = self._calc_loss(train_data, weights, save_loss=True)
                loss = sum(loss).item() if self.pcgrad else loss.item()
                epoch_counter.set_postfix_str(f'{loss:.4f}')

                if torch.isnan(torch.Tensor([loss])):
                    print('NaN loss. Re-instantiate the model and train again.')
                    break

                # Early stopping and best model:
                if loss < self.best_state['loss']:
                    patience = 0
                    self.best_state['state'] = self.net.state_dict().copy()
                    self.best_state['epoch'] = epoch
                    self.best_state['loss'] = loss
                else:
                    patience += 1
                    if 0 < early_stopping <= patience:
                        print(f'Early stopping in the {epoch} epoch.')
                        break
        
        # Post-training:
        except KeyboardInterrupt:
            # The losses record must be clipped to the last valid epoch:
            last_valid_epoch = len(self.losses['total'])
            for loss_type in ('physics', 'boundary', 'initial', 'known'):
                self.losses[loss_type] = self.losses[loss_type][:last_valid_epoch]
            
            print(f'Training interrupted at epoch {last_valid_epoch}.')
            
        if calculate_confidence:
            self.net.dropout.p = ci_config['dropout']
            self.net.confidence = ci_config['confidence']
            self._confidence_intervals(train_data, weights, samples, ci_config)
            self.net.dropout.p = 0

        
    def _confidence_intervals(self, train_data, weights, samples, ci_config):
            '''
            Computes the confidence interval with epochs given previously, as follows:
            If N > 1 are the number of epochs given to compute the confidence interval, then the
            mean solution is
            hat_u = mean(u_1, ..., n_N)
            where u_i corresponds to the discretization of the net in inter_pints by dimension, for each
            output of the net.
            Then, s is the unbiased standard deviation of the sample, T is a t-Student distribution with
            N-1 freedom degrees and c it's a real number satisfying
            Probability(-c <= T <= c) = confidence
            With this the lower bound of the confidence interval is calculated as
            L = hat_u - s*c/sqrt(N)
            and the upper bound as
            U = hat_u + s*c/sqrt(N)
            For last, the tensors of u_hat, L and U will be interpolated in the domain with Scipy interpolators.
            For ODEs, interp1d will be used and for the others RegularGridInterpolator. This functions will
            be saved in pinn.conf_interval.
            Args:
                train_data: Data to train, with collocation, boundary, initial and known if are needed.
                weigths: Weights for the looses.
                samples: Dictionary with quantity of samples per data, as physics (collocation), boundary,
                initial and known.
            '''
            def closure():
                self.optimizer.zero_grad()
                loss = self._calc_loss(train_data, weights)
                if self.pcgrad:
                    self.optimizer.pc_backward(loss)
                    return sum(loss)
                else:
                    loss.backward()
                    return loss
            var_names = self.pde.domain.variable_names
            grid = torch.stack(self.pde.domain.spatial_domain.grid(ci_config['step']))
            data = {var_name: grid[i] for (i, var_name) in enumerate(var_names)}
            u_conf = torch.zeros([ci_config['epochs'], self.net.n_output, len(grid[0])])
            self.net.train()
            try:
                bar_format = '{l_bar}{bar}| epoch {n_fmt} of {total_fmt} ({rate_fmt}) | loss: {postfix} | training time: {elapsed}'
                epoch_counter = tqdm.trange(1, ci_config['epochs'] + 1, unit=' epochs', bar_format=bar_format, miniters=1)
                train_data = self._generate_data(samples)
                for epoch in epoch_counter:
                    # Optimization:
                    self.optimizer.step(closure)
                    if self.scheduler is not None:
                        self.scheduler.step()
                    loss = self._calc_loss(train_data, weights, save_loss=True)
                    loss = sum(loss).item() if self.pcgrad else loss.item()
                    if torch.isnan(torch.Tensor([loss])):
                        print('NaN loss. Re-instantiate the model and train again.')
                        break
                    epoch_counter.set_postfix_str(f'{loss:.4f}')
                    u_conf[epoch-1] = torch.stack(self.net(data)).reshape(([self.net.n_output, len(grid[0])]))
            # Post-training:
            except KeyboardInterrupt:
                last_valid_epoch = len(self.losses['total'])
                for loss in 'physics boundary initial known'.split():
                    self.losses[loss] = self.losses[loss][:last_valid_epoch]
                print(f'Training interrupted at epoch {last_valid_epoch}.')
            M = u_conf.mean(dim=0).detach().numpy()
            S = u_conf.std(dim=0, unbiased=True).detach().numpy()
            c = stats.t.ppf((1 + ci_config['confidence']) / 2., ci_config['epochs'] - 1)
            conf = S*c/(np.sqrt(ci_config['epochs']))
            lower_int = M - conf
            upper_int = M + conf
            M_inter, Lower_inter, Upper_inter = [], [], []
            gr, interpolator = (grid[0], interpolate.interp1d) if len(var_names) == 1 else (grid.T, interpolate.RBFInterpolator)
            for i in range(self.net.n_output):
                M_inter.append(interpolator(gr, M[i]))
                Lower_inter.append(interpolator(gr, lower_int[i]))
                Upper_inter.append(interpolator(gr, upper_int[i]))
            self.net.conf_interval = (M_inter, Lower_inter, Upper_inter)
        
    
    def plot_loss(self, first_epoch: int = 1, last_epoch: int = None):
        '''
        show the recorded losses during training.

        - first_epoch: indicates from which epoch the loss will be plotted. This to avoid scale problem
            due to abrupt loss descending at the beginning.
        - last_epoch: indicates the last loss epoch that will be shown. None indicates that loss will
            be plotted until the last recorded epoch.
        '''
        
        if last_epoch is None:
            last_epoch = len(self.losses['total'])
        epochs = torch.arange(first_epoch, last_epoch + 1, 1)
        
        plt.figure(figsize=(12, 5))
        for loss in ('physics', 'boundary', 'initial', 'known', 'total'):
            linewidth, alpha, marker = (2, 1, 'o') if loss == 'total' else (1, 0.7, None)
            plot_data = self.losses[loss][first_epoch - 1: last_epoch]
            if max(plot_data) != 0:
                plt.plot(epochs, plot_data, label=loss, linewidth=linewidth, alpha=alpha, marker=marker)
        
        plt.xlabel('epoch'); plt.ylabel('loss'); plt.title('Training loss')
        plt.grid(alpha=0.3, linestyle='--'); plt.legend()
        plt.tight_layout()
        plt.show()